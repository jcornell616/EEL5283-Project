{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Nerual Network (CNN)\n",
    "##### Maxwell Rosenzweig\n",
    "\n",
    "The convolutional neural network (CNN) is designed using Tensorflow. Tensorflow and it's prerequisites must be installed (see https://www.tensorflow.org/install/pip). For HP tuning, the keras tuning module must be separately installed (see https://keras.io/keras_tuner/).\n",
    "\n",
    "Before running this file, you need to run `demo.m` and `preprocessing.ipynb` to generate `spect_train.npy`, `downsampled_acc_train.npy`, `spect_test.npy`, and `downsampled_acc_test.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tensorflow install and gpu compatability\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and format testing data\n",
    "\n",
    "Run these 2 blocks in order to load and format the training data. The 3rd block clears the original copy from memory for performance improvements. The 4 blocks after that do the same, but for testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "spect_train = np.load('../data/spect_train.npy', allow_pickle=True)\n",
    "acc_train = np.load('../data/downsampled_acc_train.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Training data\n",
    "# Align the spectograms and acc data\n",
    "# We skip the first 10 samples to get stable LFP data\n",
    "\n",
    "stacked = np.stack(acc_train[0]).transpose()\n",
    "spect_train_concat = spect_train[0][0:min(len(spect_train[0]), len(stacked))-1:]\n",
    "acc_train_concat = stacked[0:min(len(spect_train[0]), len(stacked))-1:]\n",
    "for i in range(1, len(spect_train)):\n",
    "    # print(f'Dataset {i} has {len(spect_train[i])} inputs and {len(np.stack(acc_train[i]).transpose())} outputs')\n",
    "    # print(f'acc shape is {np.shape(np.stack(acc_train[i]))}')\n",
    "    stacked = np.stack(acc_train[i]).transpose()\n",
    "    new_spect = spect_train[i][0:min(len(spect_train[i]), len(stacked))-1:]\n",
    "    stacked = stacked[0:min(len(spect_train[i]), len(stacked))-1:]\n",
    "    # throw out any data with outliers or too few data points\n",
    "    if (len(new_spect) == 0):\n",
    "        print(f'Dataset {i} has only {len(spect_train[i])} inputs and {len(acc_train[i].transpose())} outputs')\n",
    "    elif (np.amax(new_spect) > 2e-6):\n",
    "        print(f'Dataset {i} contains an outlier.')\n",
    "    else:\n",
    "        spect_train_concat = np.concatenate((spect_train_concat, new_spect))\n",
    "        acc_train_concat = np.concatenate((acc_train_concat, stacked))\n",
    "\n",
    "spect_train1 = spect_train_concat[...,0]\n",
    "spect_train2 = spect_train_concat[...,1]\n",
    "\n",
    "# Bring values to near [0,1.0] range\n",
    "spect_train1 = (spect_train1 * 5e5)\n",
    "spect_train2 = (spect_train2 * 5e5)\n",
    "\n",
    "print(np.shape(spect_train1))\n",
    "print(np.shape(spect_train2))\n",
    "print(np.shape(acc_train_concat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checks\n",
    "\n",
    "print(np.median(spect_train1), np.amax(spect_train1), np.amin(spect_train1), np.std(spect_train1))\n",
    "print(np.median(spect_train2), np.amax(spect_train2), np.amin(spect_train2), np.std(spect_train2))\n",
    "print(np.median(acc_train_concat), np.amax(acc_train_concat), np.amin(acc_train_concat), np.std(acc_train_concat))\n",
    "\n",
    "assert not np.any(np.isnan(spect_train1))\n",
    "assert not np.any(np.isnan(spect_train2))\n",
    "assert not np.any(np.isnan(acc_train_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Free up memory from raw inputs\n",
    "del spect_train\n",
    "del acc_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing data\n",
    "spect_test = np.load('../data/spect_test.npy', allow_pickle=True)\n",
    "acc_test = np.load('../data/downsampled_acc_test.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape testing data\n",
    "# Throw away first 7 acc values; these correspond to the data points before 400ms\n",
    "\n",
    "stacked = np.stack(acc_test[0]).transpose()\n",
    "spect_test_concat = spect_test[0][:min(len(spect_test[0]), len(stacked))-1:]\n",
    "acc_test_concat = stacked[:min(len(spect_test[0]), len(stacked))-1:]\n",
    "for i in range(1, len(spect_test)):\n",
    "    # print(f'Dataset {i} has {len(spect_test[i])} inputs and {len(np.stack(acc_test[i]).transpose())} outputs')\n",
    "    # print(f'acc shape is {np.shape(np.stack(acc_test[i]))}')\n",
    "    stacked = np.stack(acc_test[i]).transpose()\n",
    "    new_spect = spect_test[i][:min(len(spect_test[i]), len(stacked))-1:]\n",
    "    stacked = stacked[:min(len(spect_test[i]), len(stacked))-1:]\n",
    "    # throw out any data with outliers or too few data points\n",
    "    if (len(new_spect) == 0):\n",
    "        print(f'Dataset {i} has only {len(spect_test[i])} inputs and {len(acc_test[i].transpose())} outputs')\n",
    "    elif (np.amax(new_spect) > 2e-6):\n",
    "        print(f'Dataset {i} contains an outlier.')\n",
    "    else:\n",
    "        spect_test_concat = np.concatenate((spect_test_concat, new_spect))\n",
    "        acc_test_concat = np.concatenate((acc_test_concat, stacked))\n",
    "\n",
    "spect_test1 = spect_test_concat[...,0]\n",
    "spect_test2 = spect_test_concat[...,1]\n",
    "spect_test1 = (spect_test1 * 5e5)\n",
    "spect_test2 = (spect_test2 * 5e5)\n",
    "\n",
    "[spect_test1, spect_val1] = np.array_split(spect_test1, 2)\n",
    "[spect_test2, spect_val2] = np.array_split(spect_test2, 2)\n",
    "[acc_test_concat, acc_val_concat] = np.array_split(acc_test_concat, 2)\n",
    "print(np.shape(spect_test1))\n",
    "print(np.shape(spect_val1))\n",
    "print(np.shape(spect_test2))\n",
    "print(np.shape(spect_val2))\n",
    "print(np.shape(acc_test_concat))\n",
    "print(np.shape(acc_val_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checks\n",
    "\n",
    "print(np.median(spect_test1), np.amax(spect_test1), np.amin(spect_test1), np.std(spect_test1))\n",
    "print(np.median(spect_test2), np.amax(spect_test2), np.amin(spect_test2), np.std(spect_test2))\n",
    "print(np.median(acc_test_concat), np.amax(acc_test_concat), np.amin(acc_test_concat), np.std(acc_test_concat))\n",
    "\n",
    "assert not np.any(np.isnan(spect_test1))\n",
    "assert not np.any(np.isnan(spect_test2))\n",
    "assert not np.any(np.isnan(acc_test_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Free up memory from raw inputs\n",
    "del spect_test\n",
    "del acc_test\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "Running this block will reset any training progress. Only run if you do not want to continue to test/fit the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel1 = Input(shape=(51, 46, 1))\n",
    "channel2 = Input(shape=(51, 46, 1))\n",
    "\n",
    "cnn1 = layers.Conv2D(filters=16, kernel_size=(1,46), activation='relu')(channel1)\n",
    "cnn1 = layers.Conv2D(filters=32, kernel_size=(3,1), activation='relu')(cnn1)\n",
    "cnn1 = layers.MaxPool2D(pool_size=(2,1))(cnn1)\n",
    "cnn1 = layers.Conv2D(filters=32, kernel_size=(3,1), activation='relu')(cnn1)\n",
    "cnn1 = layers.MaxPool2D(pool_size=(2,1))(cnn1)\n",
    "cnn1 = layers.Flatten()(cnn1)\n",
    "\n",
    "cnn2 = layers.Conv2D(filters=16, kernel_size=(1,46), activation='relu')(channel2)\n",
    "cnn2 = layers.Conv2D(filters=32, kernel_size=(3,1), activation='relu')(cnn2)\n",
    "cnn2 = layers.MaxPool2D(pool_size=(2,1))(cnn2)\n",
    "cnn2 = layers.Conv2D(filters=32, kernel_size=(3,1), activation='relu')(cnn2)\n",
    "cnn2 = layers.MaxPool2D(pool_size=(2,1))(cnn2)\n",
    "cnn2 = layers.Flatten()(cnn2)\n",
    "\n",
    "\n",
    "output = layers.concatenate([cnn1, cnn2])\n",
    "output = layers.Dense(512, activation='relu')(output)\n",
    "output = layers.Dense(256, activation='relu')(output)\n",
    "output = layers.Dense(128, activation='relu')(output)\n",
    "output = layers.Dense(3)(output)\n",
    "\n",
    "model = models.Model(inputs=[channel1, channel2], outputs=output)\n",
    "opt = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model\n",
    "Run to fit the model. The second block will display the training history after the model has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import *\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "earlystop2 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "history = model.fit(\n",
    "    (spect_train1, spect_train2),\n",
    "    acc_train_concat,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystop, earlystop2],\n",
    "    validation_data=((spect_val1, spect_val2), acc_val_concat),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss history plot\n",
    "plt.plot(history.history['loss'][:])\n",
    "plt.plot(history.history['val_loss'][:])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.title('Model error')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model\n",
    "Optionally run the first block to load a model from file. The second block will run the model on the testing set. The following 3 will display the predicitons vs the true values of the accelerometer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate((spect_test1, spect_test2), acc_test_concat, batch_size=256)\n",
    "prediciton = model.predict((spect_test1, spect_test2))\n",
    "prediction = np.array(prediciton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_test = acc_test_concat.T[0]\n",
    "data_x_pred = prediction[...,0]\n",
    "\n",
    "plt.plot(data_x_pred, label='x prediciton')\n",
    "plt.plot(data_x_test, label='x test')\n",
    "plt.title(\"X Prediction vs Test\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Voltage')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_y_test = acc_test_concat.T[1]\n",
    "data_y_pred = prediction[...,1]\n",
    "plt.plot(data_y_pred, label='y prediciton')\n",
    "plt.plot(data_y_test, label='y test')\n",
    "plt.title(\"Y Prediction vs Test\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Voltage')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_z_test = acc_test_concat.T[2]\n",
    "data_z_pred = prediction[...,2]\n",
    "plt.plot(data_z_pred, label='z prediciton')\n",
    "plt.plot(data_z_test, label='z test')\n",
    "plt.title(\"Z Prediction vs Test\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Voltage')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Hyperparameters\n",
    "Be sure to import both the training and testing set to run this. Takes a long time, recommend leaving overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import *\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    channel1 = Input(shape=(51, 46, 1))\n",
    "    channel2 = Input(shape=(51, 46, 1))\n",
    "\n",
    "    input_drop_rate = hp.Float(f'dropout_conv_rate', 0, 0.2, step=0.05, default=0.1)\n",
    "    cnn1 = layers.Dropout(input_drop_rate)(channel1)\n",
    "    cnn2 = layers.Dropout(input_drop_rate)(channel2)\n",
    "\n",
    "    hp_filters = hp.Int('filters_vert', 8, 64, step=8, default=32)\n",
    "    hp_kernel_size = hp.Int('kernel_size_vert', 3, 46, step=1, default=32)\n",
    "    cnn1 = layers.Conv2D(filters=hp_filters, kernel_size=(1,hp_kernel_size), activation='relu')(cnn1)\n",
    "    cnn2 = layers.Conv2D(filters=hp_filters, kernel_size=(1,hp_kernel_size), activation='relu')(cnn2)\n",
    "\n",
    "    for i in range(hp.Int('conv_layers', 1, 3, default=2)):\n",
    "        hp_filters = hp.Int(f'filters_{i}', 8, 64, step=8, default=32)\n",
    "        hp_kernel_size = hp.Int(f'kernel_size_{i}', 3, 5, step=2, default=3)\n",
    "        cnn1 = layers.Conv2D(\n",
    "            filters=hp_filters,\n",
    "            kernel_size=(hp_kernel_size, 1),\n",
    "            activation='relu',\n",
    "            padding='same')(cnn1)\n",
    "        cnn2 = layers.Conv2D(\n",
    "            filters=hp_filters,\n",
    "            kernel_size=(hp_kernel_size, 1),\n",
    "            activation='relu',\n",
    "            padding='same')(cnn2)\n",
    "        cnn1 = layers.MaxPooling2D(pool_size=(2,1))(cnn1)\n",
    "        cnn2 = layers.MaxPooling2D(pool_size=(2,1))(cnn2)\n",
    "\n",
    "    conv_drop_rate = hp.Float(f'dropout_conv_rate', 0, 0.2, step=0.05, default=0.1)\n",
    "    cnn1 = layers.Dropout(conv_drop_rate)(cnn1)\n",
    "    cnn2 = layers.Dropout(conv_drop_rate)(cnn2)\n",
    "    cnn1 = layers.Flatten()(cnn1)\n",
    "    cnn2 = layers.Flatten()(cnn2)\n",
    "\n",
    "    output = layers.concatenate([cnn1, cnn2])\n",
    "    dense_units = hp.Int(f'dense_units', 64, 768, step=64, default=512)\n",
    "    dense_drop_rate = hp.Float(f'dropout_dense_rate_{i}', 0, 0.5, step=0.1, default=0.3)\n",
    "    for i in range(hp.Int('dense_layers', 1, 5, default=3)):\n",
    "        output = layers.Dense(units=dense_units, activation='relu')(output)\n",
    "        output = layers.Dropout(dense_drop_rate)(output)\n",
    "    output = layers.Dense(3)(output)\n",
    "\n",
    "    model = models.Model(inputs=[channel1, channel2], outputs=output)\n",
    "    opt = optimizers.Adam(learning_rate=hp.Float('learning_rate',min_value=0.0005, max_value=0.005, sampling='log', default=0.001))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    max_trials=500,\n",
    "    objective=\"val_loss\"\n",
    ")\n",
    "\n",
    "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs)\n",
    "earlystop_loss = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "earlystop_val_loss = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "tuner.search(\n",
    "    (spect_train1, spect_train2), acc_train_concat,\n",
    "    epochs=300,\n",
    "    validation_data=((spect_val1, spect_val2), acc_val_concat),\n",
    "    shuffle=True,\n",
    "    callbacks=[earlystop_loss, earlystop_val_loss, tboard_callback]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
